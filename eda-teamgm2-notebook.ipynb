{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#   EDSA: MOVIE RECOMMENDATION 2022  \n*unsupervised learning*\n\n¬© Explore Data Science Academy\n\n#  2201FT_Team GM2  \n\nHonour Code\nWe {***Team GM2***}, confirm - by submitting this document - that the solutions in this notebook are a result of our own work and that we abide by the EDSA honour code.\n\nNon-compliance with the honour code constitutes a material breach of contract.\n\n#  MEET THE TEAM!  \n[Lindiwe Songelwa](https://www.linkedin.com/in/lindiwe-songelwa/)\n\n[Thabang Pardon Mokoena](https://www.linkedin.com/in/thabang-pardon-mokoena/)\n\n[Sizakele Beauty Mtsweni](https://www.linkedin.com/in/sizakele-beuty-mtsweni-236935172/)\n\n[Nkoka Khosa](https://www.linkedin.com/)\n\n[Elizabeth Matlala](https://www.linkedin.com/)\n\n[Kanego Kgabalo](https://www.linkedin.com/)\n\n\n# PREDICT OVERVIEW\n\n*Recommender systems are the systems that are designed to recommend things to the user based on many different factors, they deal with a large volume of information present by filtering the most important information based on the data provided by a user and other factors that take care of the user‚Äôs preference and interest.*\n \n *Having the ability to predict user ratings, even before the user has provided one, makes recommender systems a powerful tool. The recommender system analyzes and finds items with similar user engagement data by filtering. It uses different analysis methods such as batch analysis, real-time analysis, or near-real-time system analysis.*\n \n *Recommender systems can be a powerful tool for any e-commerce business, and rapid future developments in the field will increase their business value even further.*\n\n\n#  PROBLEM STATEMENT \n\n> Create a collaborative or content based filtering algorithm that will predict how a user will rate a movie they have not yet seen\n ","metadata":{}},{"cell_type":"markdown","source":"![](https://wallpaperaccess.com/full/4839516.jpg)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T07:40:21.264642Z","iopub.execute_input":"2022-07-21T07:40:21.265692Z","iopub.status.idle":"2022-07-21T07:40:22.055797Z","shell.execute_reply.started":"2022-07-21T07:40:21.265572Z","shell.execute_reply":"2022-07-21T07:40:22.054385Z"}}},{"cell_type":"markdown","source":"<a id=\"cont\"></a>\n\n## <u> Table of Contents </u> \n\n<a href=#one>1. Importing Packages</a>\n\n<a href=#two>2. Important Functions</a>\n\n<a href=#three>3. Loading and Pre-processing Data</a>\n\n<a href=#four>4. Exploratory Data Analysis (EDA)</a>\n\n<a href=#five>5. Model Explanations</a>\n\n<a href=#six>6. Modelling</a>\n\n<a href=#seven>7. Model Performance / Validation</a>\n\n<a href=#eight>8. Reference</a>","metadata":{}},{"cell_type":"markdown","source":" <a id=\"one\"></a>\n## 1. Importing Packages\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| üìΩ Description: Importing Packages üìΩ |\n| :--------------------------- |\n| In this section we will import, and briefly discuss, the libraries that will be used throughout our analysis and modelling. |\n\n---","metadata":{}},{"cell_type":"code","source":"# Import our regular old heroes \nimport numpy as np\nimport pandas as pd\nimport scipy as sp # <-- The sister of Numpy, used in our code for numerical efficientcy. \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot \nfrom PIL import Image\nimport matplotlib.ticker as ticker\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nimport re\n\n# Entity featurization and similarity computation\nfrom sklearn.metrics.pairwise import cosine_similarity \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#NLP libraries\nfrom nltk.corpus import stopwords\nimport string\nimport nltk\n\n# Models\nfrom surprise import Reader, Dataset\nfrom surprise import SVD, NormalPredictor, BaselineOnly, NMF, SlopeOne, CoClustering,KNNWithMeans,KNNBasic,accuracy\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom surprise.model_selection import cross_validate,train_test_split\n \n# Performance Evaluation\nfrom surprise import accuracy\nfrom sklearn.metrics import mean_squared_error\nfrom surprise.model_selection import GridSearchCV, cross_validate, train_test_split\n\n# Display\n%matplotlib inline\nsns.set(font_scale=1)\nsns.set_style(\"white\")\npd.set_option('display.max_columns', 37)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This package makes the algorithm run faster\n!pip install git+https://github.com/Explore-Unsupervised-Learning-TEAM-GM2/Surprise","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\"> \n <b>Data description:</b> \n\nThis dataset consists of several million 5-star ratings obtained from users of the online MovieLens movie recommendation service. The MovieLens dataset has long been used by industry and academic researchers to improve the performance of explicitly-based recommender systems, and now you get to as well!\n\nFor this Predict, we'll be using a special version of the MovieLens dataset which has enriched with additional data, and resampled for fair evaluation purposes.\n\nOur datasets is divided into numeric and categorical data, where;\n* Categorical data is defined as a type of data that is used to group information with similar characteristics while\n* Numerical data is defined a type of data that expresses information in the form of numbers.\n</div>     ","metadata":{}},{"cell_type":"markdown","source":" <a id=\"two\"></a>\n## 2. Important Functions\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| üìΩ Description: Important Functions üìΩ |\n| :--------------------------- |\n| In this section we are coding important functions that will be used later in the notebook. |\n\n---\n","metadata":{}},{"cell_type":"code","source":"#defining a function for removing punctuations\n\npunctuations = string.punctuation \n\ndef remove_punctuation(text):\n    \n    '''removes punctuation and any special characters from a text .\n   Args:\n        text (str) : containing text data\n    \n    returns:\n        dataframe(df): dataframe with tokens within the specified column.\n    '''\n\n    return text.translate(str.maketrans(' ',' ', punctuations))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing numbers\ndef remove_numbers(df, colname):\n    ''' Function removes numbers using regex search\n    Arg:\n        dataframe (df): dataframe that has a text column\n        colname(str): name of the column with text data and emojis\n    return\n        dataframe (df): dataframe with no numbers from the text data\n    '''\n   \n    num_pattern = r'[0-9]'\n    df[colname] = df[colname].replace(to_replace = num_pattern, value = '', regex = True)\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing letters\ndef remove_letters(df, colname):\n    ''' Function removes letters using regex search\n    Arg:\n        dataframe (df): dataframe that has a text column\n        colname(str): name of the column with text data and emojis\n    return\n        dataframe (df): dataframe with no letters from the text data\n    '''\n   \n    num_pattern = r'^$|[a-zA-Z]|Œ§œÉŒπœÑœÉŒ¨ŒΩŒ∑œÇ|101Ê¨°Ê±ÇÂ©ö|2006‚Äì2007|–≤—ã–±—ã–≤–∞–Ω–∏–µ|ŸæÿØÿ±|–ù–∞—á–∞–ª—å–Ω–∏–∫|–î–∂–∞|–î–µ–≤–æ—á–∫–∏|–ø–µ—Ä–≤–æ–≥–æ'\n    df[colname] = df[colname].replace(to_replace = num_pattern, value = np.nan, regex = True)\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Counts the number of user ratings\ndef user_ratings(df, n):\n    plt.figure(figsize=(8,6))\n    temp_df = df['userId'].value_counts().head(n)\n    ax = sns.barplot(x = temp_df.index, y = temp_df, order = temp_df.index, palette = 'bright', edgecolor = \"black\")\n    for i in ax.patches:\n        ax.text(i.get_x() + i.get_width()/2., i.get_height(), '%d' % int(i.get_height()), fontsize=11, ha = 'center', va = 'bottom')\n    plt.title(f'Top {n} Users by Ratings', fontsize = 14)\n    plt.xlabel('User ID')\n    plt.ylabel('Number of Ratings')\n    print(\"Number of ratings:\\t\",df['userId'].value_counts().head(n).sum(),\n         \"\\nTotal number of movies:\\t\\t\", df['movieId'].nunique())\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ratings_dist(df, column='rating'):\n    plt.figure(figsize=(30,50))\n    ax = sns.displot(df[f'{column}'],bins=10, kde = False, color = \"red\")\n    mean = df[f'{column}'].mean()\n    median = df[f'{column}'].median()\n    plt.axvline(x = mean, label = f'mean {round(mean,2)}' , color = '#1726A0', lw = 3, ls = '--')\n    plt.axvline(x = median, label = f'median {median}' , color = '#FFFF00', lw = 3, ls = '--')\n    plt.xlim((0.5,5))\n    plt.ylim((0,2500000))\n    plt.title(f'Distribution of Ratings\\n', fontsize = 12)\n    plt.xlabel('Rating')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_ratings(count, n, color = 'red', best = True, method = 'mean'):\n    if method == 'mean':\n        movie_avg_rates = pd.DataFrame(train.merge(movies, on = 'movieId', how = 'left').groupby(['movieId', 'title'])['rating'].mean())\n    else:\n        movie_avg_rates = pd.DataFrame(train.merge(movies, on = 'movieId', how = 'left').groupby(['movieId', 'title'])['rating'].median())\n    movie_avg_rates['count'] = train.groupby('movieId')['userId'].count().values\n    movie_avg_rates.reset_index(inplace = True)\n    movie_avg_rates.set_index('movieId', inplace = True)\n\n    # Remove movies that have been rated fewer than n times\n    movie_data = movie_avg_rates[movie_avg_rates['count'] > count]\n    movie_data.sort_values('rating', inplace = True, ascending = False)\n    if best == True:\n        plot = movie_data.head(n).sort_values('rating', ascending = True)\n        title = 'best rated'\n    else:\n        plot = movie_data.tail(n).sort_values('rating', ascending = False)\n        title = 'worst rated'\n    plt.figure(figsize = (10,9))\n    sns.scatterplot(x = plot['rating'], y = plot['title'], size = plot['count'], color = color)\n    plt.xlabel('rating')\n    #plt.ylabel(off)\n    plt.tick_params(axis = 'y', which ='both', labelleft = False, labelright = True)\n    plt.title(f'Top {n} {title} movies with over {count} ratings', fontsize = 14)\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <a id=\"three\"></a>\n## 3. Loading and Pre-processing Data\n<a href=#cont>Back to Table of Contents</a>\n\n---\n    \n| üìΩ Description: Importing Packages & Pre-processing üìΩ |\n| :--------------------------- |\n| In this section we are importing all the csv files with the use of the pandas library that contain our movie's data and converted them into dataframes for our analysis.\n  Perform summary statistics on the datasets; checking for null values, descriptive statistics and summary statistics|\n\n---\n","metadata":{}},{"cell_type":"code","source":"#load datasets\ntrain = pd.read_csv('../input/edsa-movie-recommendation-2022/train.csv')\ntest = pd.read_csv('../input/edsa-movie-recommendation-2022/test.csv')\nmovies = pd.read_csv('../input/edsa-movie-recommendation-2022/movies.csv')\n# specify which columns to include by name\nimdb = pd.read_csv('../input/edsa-movie-recommendation-2022/imdb_data.csv', usecols=['movieId','title_cast', 'director', 'runtime'])\ntags = pd.read_csv('../input/edsa-movie-recommendation-2022/tags.csv')\nprint (f'Number of rows in train set: {train.shape[0]}')\nprint (f'Number of rows in test set: {test.shape[0]}')\nprint (f'Number of rows in movies test: {movies.shape[0]}')\nprint (f'Number of rows in imdb set: {imdb.shape[0]}')\nprint (f'Number of rows in tags set: {tags.shape[0]}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\"> \n <b>why we are merging the datasets:</b>\n    \nwe combined them into one dataset, for the purpose of eda. However since we have multiple csv files which do not contribute to our final model in anyway, we will leave the train and test data as is. The following sections until modeling will only focus on the additional csv files that we will use in our web app\n</div>        ","metadata":{}},{"cell_type":"code","source":"#merge datasets\nmovies = pd.merge(movies,imdb,on=\"movieId\")\nmovies = pd.merge(movies,tags,on=\"movieId\")\n\n#view new dataset\nmovies.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a quick overview of the data\nprint(\"Movies Dataset Info Total Rows | Total Columns | Total Null Values \\n\")\nprint(movies.info())\nprint(\"---------------------------------------------------------------------\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\"> \n <b>What we have learnt so far:</b> \n    \n* The runtime has an average of 100.31 minutes with a minimum of 1 minute and a maximum of 877 minutes.\n    \n* 75% of the runtime is less than 109 minutes with a median of 98 minutes.\n\n*  large number of feaures lead to overfitting we have to perform feature reduction and/or dimensionality reduction\n    \n* some features are redundant; they are not necessary for modeling    \n    \n </div>      ","metadata":{}},{"cell_type":"code","source":"#checking null values\n\nprint('\\nSummary of movies data missing values\\n')\nprint('{}\\n'.format(movies.isna().sum()))\n\nprint('\\nSummary of test data missing values\\n')\nprint('{}\\n'.format(test.isna().sum()))\n\nprint('\\nSummary of train data missing values\\n')\nprint('{}\\n'.format(train.isna().sum()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = movies.title_cast.isna().sum()\nprint(f'Missing Values: {missing_values}  |   Percentage: {round(( missing_values/ movies.title_cast.shape[0]) *100, 2)}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = movies.director.isna().sum()\nprint(f'Missing Values: {missing_values}  |   Percentage: {round(( missing_values/ movies.director.shape[0]) *100, 2)}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = movies.runtime.isna().sum()\nprint(f'Missing Values: {missing_values}  |   Percentage: {round(( missing_values/ movies.runtime.shape[0]) *100, 2)}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = movies.tag.isna().sum()\nprint(f'Missing Values: {missing_values}  |   Percentage: {round(( missing_values/ movies.tag.shape[0]) *100, 2)}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove column name 'timestamp'\nmovies = movies.drop(['timestamp'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Datacleaning","metadata":{}},{"cell_type":"code","source":"#creating a new column for year\nmovies['year'] = movies.title.str.extract('(\\(\\d\\d\\d\\d\\))',expand=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies['year'] = movies.year.str.extract('(\\d\\d\\d\\d)',expand=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies['title'] = movies.title.str.replace('(\\(\\d\\d\\d\\d\\))', '')\nmovies.head()                                           ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datacleaning\n\n#removing punctuations from title \nmovies.title = movies.title.apply(remove_punctuation)\n\n#removing punctuations from genres\nmovies.genres = movies.genres.str.replace('|',' ',regex = True)\nmovies.genres = movies.genres.apply(remove_punctuation)\n\n#removing punctuations from title_cast\nmovies.title_cast = movies.title_cast.str.replace('|',' ',regex = True)\nmovies.title_cast = movies.title_cast.astype(str).apply(remove_punctuation)\n\n#removing punctuations from director\nmovies.director = movies.director.str.replace('|',' ',regex = True)\nmovies.director = movies.director.astype(str).apply(remove_punctuation)\nmovies.director = movies.director.dropna()\n\nmovies.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = movies.year.isna().sum()\nprint(f'Missing Values: {missing_values}  |   Percentage: {round(( missing_values/ movies.year.shape[0]) *100, 2)}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"> \n <b>What we have learnt so far:</b> \n\n    \n1. The train data has:\n*  <b>*10000038</b>  rows*\n*  no missing values\n* four columns    \n    \n2. The test data has:\n*  <b>*5000019</b>  rows*\n*  two columns   \n    \n3. The object datatypes are the only columns with empty rows\n    \n4. We have multiple datasets with multiple features:\n*  irrelevant features introduce noise \n*  some features are redundant\n*  large number of feaures lead to overfitting\n    \n**We have to perform feature reduction and/or dimensionality reduction**    \n        \n</div>         ","metadata":{}},{"cell_type":"markdown","source":" <a id=\"four\"></a>\n## 4. Exploratory Data Analysis (EDA)\n<a href=#cont>Back to Table of Contents</a>\n\n| üìΩ *Description: Exploratory Data Analysis* üìΩ |\n| :--------------------------- |\n  In this section, we will perform an in-depth analysis of all the variables in the dataframe. \n  We analyse the data using visual techniques,this is used to discover trends, patterns,\n  or to check assumptions with the help of statistical summary and graphical representations.\n  Essentially, we want to see how everything is related to each other","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>P.S:</b> for in this notebook we will be focusing more on the features related to the movie and how they compare to each other.\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"### Ratings\n*Check the rating of the movies for any insights*","metadata":{}},{"cell_type":"code","source":"movies['runtime'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the funtion for this cell is available in section 2 of the notebook\nuser_ratings(train,10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['rating'].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['rating'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the funtion for this cell is available in section 2 of the notebook\nratings_dist(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_ratings(10000, 10, 'red', True, 'mean')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_ratings(500, 10,'blue', False, 'mean')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"> \n <b>What we have learnt so far:</b>\n    \n* The rating scale is between 0.5 and 4.0\n* Average rating is 3.5    \n* Highest rated movie is Shawshank Redemption\n* Lowest rated movie is Battlefield Earth    \n</div>    ","metadata":{}},{"cell_type":"markdown","source":"### Directors\n*Check the most popular directors for insights*","metadata":{}},{"cell_type":"code","source":"#group director by rating\ndirector = df.groupby(['director'])['rating'].sum().reset_index().sort_values('rating', ascending=False).head(10)\ndirector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(director, x = 'rating', y = 'director', title= 'Top Rated Director',color='director', labels={\"director\": \"Director\", \"rating\": \"Number of ratings\"})\nfig.update_layout(yaxis_categoryorder = 'total ascending')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"> \n <b>What we have learnt so far:</b>\n    \n* Top rated director is Quentin Tarantino\n* Steven Spielberg has the least ratings\n* Oddly, J.R.R Tolken, the South African author of The Lord of The Rings trilogies as well as The Hobbit books is listed as a director   \n</div>","metadata":{}},{"cell_type":"markdown","source":"### Movies\n*Check the most popular movie yearly for insights*","metadata":{}},{"cell_type":"code","source":"# group by title to see which titles are most present in the dataframe\ntitle = df.groupby(['title'])['title'].count().reset_index(name='Count').sort_values('Count', ascending=False).head(10)\ntitle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(title, x = 'Count', y = 'title', title= 'Top Movies',color='title', labels={\"title\": \"Titles\", \"Count\": \"Number of appearances\"})\nfig.update_layout(yaxis_categoryorder = 'total ascending')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nfig = px.pie(df, values='title_cast', names='title_cast', title='Title Distribution')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.sunburst(df, path=['genres', 'director'], values='rating',\n                  color='movieId', hover_data=['title'])\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\ndef wordCloud_generator(df, title=None):\n    wordcloud = WordCloud(width = 1000, height = 800,\n                          background_color ='white',\n                          min_font_size = 10\n                         ).generate(\" \".join(df.values))                      \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud, interpolation='bilinear') \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.title(title,fontsize=30)\n    plt.show()\nwordCloud_generator(df['title'], title=\"Top Titles\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"> \n <b>What we have learnt so far:</b>\n    \n* Top rated movies are all American\n* Top rated movie are what film buffs consider classics\n* Top 3 rated movies were released in the early 90s\n* Top rated genre or genre combo is Drama    \n    \n</div>","metadata":{}},{"cell_type":"markdown","source":" <a id=\"five\"></a>\n## 5. Model Explanations\n<a href=#cont>Back to Table of Contents</a>\n\n| üìΩ *Description: Modeling* üìΩ |\n| :--------------------------- |\n  In this section, we are explaining the models that are used in this notebook.\n  \n  ---","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>P.S:</b>\nCollaborative Filtering (CF) is a method of making automatic predictions about the interests of a user by learning its preferences (or taste) based on information of his engagements with a set of available items, along with other users‚Äô engagements with the same set of items. \n\nIn other words, CF assumes that, if a person A has the same opinion as person B on some set of issues X={x1,x2,‚Ä¶}, then A is more likely to have B‚Äòs opinion on a new issue y than to have the opinion of any other person that doesn‚Äôt agree with A on X\n\n\nCF algorithms are able to infer users‚Äô hidden preferences and to exploit those preferences to recommend them new potentially-good items, algorithms are best known for their use on e-commerce web sites, where they serve as cornerstones for their recommendation-engines\n</div>","metadata":{}},{"cell_type":"markdown","source":"### SVD\n\n* Set of supervised learning methods used for classification, regression and outliers detection.\n* Effective in high dimensional spaces.\n* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n\n <a href='#svd'>click to view code</a>","metadata":{}},{"cell_type":"markdown","source":"![](https://www.askpython.com/wp-content/uploads/2020/11/SVD-1.jpg.webp)","metadata":{}},{"cell_type":"markdown","source":"### Normal Predicator\n\n * Prediction algorithm available for recommendation\n * Algorithm predicting a random rating based on the distribution of the training set, which is assumed to be normal. \n \n <a href='#np'>click to view code</a>","metadata":{}},{"cell_type":"markdown","source":"### Baseline Only\n\n* Algorithm predicting the baseline estimate for given user and item.\n* Common baseline models include linear regression when predicting continuous values, logistic regression when classifying structured data, pretrained convolutional neural networks for vision related tasks, and recurrent neural networks and gradient boosted trees for sequence modeling\n\n<a href='#BO'>click to view code</a>","metadata":{}},{"cell_type":"markdown","source":"### Non-negative Matrix Factorization\n\n* The objective of NMF is dimensionality reduction and feature extraction\n* Intuitively, NMF assumes that the original input is made of a set of hidden features, represented by each column of W matrix and each column in H matrix represents the ‚Äòcoordinates of a data point‚Äô in the matrix W.\n* Is used in major applications such as image processing, text mining, spectral data analysis and many more\n\n<a href='#nmf'>click to view code</a>\n","metadata":{}},{"cell_type":"markdown","source":"### Co-Clustering (or Biclustering) \n\n* Simultaneous clustering of the rows and columns of a matrix. \n* Can be seen as a method of co-grouping two types of entities simultaneously, based on similarity of their pairwise interactions.\n* Extremely useful when the above mentioned pairwise interactions signal is sparse\n\n<a href='#co_clustering'>click to view code</a>","metadata":{}},{"cell_type":"markdown","source":"### Slope One\n\n* A simple yet accurate collaborative filtering algorithm\n* Slope One algorithm uses simple linear regression model to solve data sparisity problem.\n* Used mostly in collaborative filtering\n\n<a href='#slope_one'>click to view code</a>\n\n","metadata":{}},{"cell_type":"markdown","source":" <a id=\"six\"></a>\n## 6. Modeling\n<a href=#cont>Back to Table of Contents</a>\n\n| üìΩ *Description: Model Performance* üìΩ |\n| :--------------------------- |\n  In this section, we peform the modeling with the train and test sets, then pick the best performing model.\n  \n  ---","metadata":{}},{"cell_type":"markdown","source":"![](https://www.bbvaapimarket.com/wp-content/uploads/2016/04/cibbva_modelo.png)","metadata":{}},{"cell_type":"code","source":"# Load the train dataset\nfrom surprise import Reader, Dataset\nreader = Reader()\nmodel = Dataset.load_from_df(train[['userId', 'movieId', 'rating']], reader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample random trainset and testset\ntrain_set,test_set = train_test_split(model,test_size=0.25,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Singular Value Decomposition (SVD)\n<a id='svd'></a>","metadata":{}},{"cell_type":"code","source":"# svd\nsvd = SVD(n_epochs = 30, n_factors = 200, init_std_dev = 0.05, random_state=42)\nsvd.fit(trainset)\npred = svd.test(testset)\nsvd_rmse = accuracy.rmse(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normal Predictor\n<a id='np'></a>","metadata":{}},{"cell_type":"code","source":"# np\nnp = NormalPredictor()\nnp.fit(trainset)\npred = np.test(testset)\nnp_rmse = accuracy.rmse(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Baseline Only\n<a id='BO'></a>","metadata":{}},{"cell_type":"code","source":"# baseline\nbsl = {'method': 'sgd','n_epochs': 40}\nBO = BaselineOnly()\nBO.fit(trainset)\npred = BO.test(testset)\nBO_rmse = accuracy.rmse(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### Non-negative Matrix Factorization\n \n <a id='nmf'></a>","metadata":{}},{"cell_type":"code","source":"# nmf\nnmf = NMF()\nnmf.fit(trainset)\npred = nmf.test(testset)\nnmf_rmse = accuracy.rmse(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Slope One\n\n<a id='slope_one'></a>","metadata":{}},{"cell_type":"code","source":"# slope one\nslope = SlopeOne()\nslope.fit(trainset)\npred = slope.test(testset)\nslope_rmse = accuracy.rmse(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Co-Clustering\n<a id='co_clustering'></a>","metadata":{}},{"cell_type":"code","source":"# co-clustering\nc_cluster = CoClustering(random_state=42)\nc_cluster.fit(trainset)\npred = c_cluster.test(testset)\ncluster_rmse = accuracy.rmse(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <a id=\"seven\"></a>\n## 7. Model Performance / Validation\n<a href=#cont>Back to Table of Contents</a>\n\n| üìΩ *Description: Model Performance* üìΩ |\n| :--------------------------- |\n  In this section, we analyse the peformance of all the models we have coded.\n  \n  ---\n\n","metadata":{}},{"cell_type":"code","source":"# Compare RMSE values between models\nfig,axis = plt.subplots(figsize=(8, 5))\nclusters = ['SVD','NormalPredictor','BaselineOnly','NMF','SlopeOne','CoClustering']\nrmse_y = [svd_rmse, np_rmse, BO_rmse, nmf_rmse, slope_rmse, cluster_rmse]\nax = sns.barplot(x = clusters, y = rmse_y, palette = 'bright', edgecolor = 'black')\nplt.title('Model Performance', fontsize = 12)\nplt.xticks(rotation=90)\nplt.ylabel('RMSE')\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()/2, p.get_y() + p.get_height(), round(p.get_height(),2), fontsize=12, ha=\"center\", va='bottom')\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"> \n <b>What we have learnt:</b>\n    \n  *  In statistics, lower values of RMSE indicate better fit. RMSE is a good measure of how accurately the model predicts the response.\n  * This metric that tells us the average distance between the predicted values from the model and the actual values in the dataset.\n  *  The lower the RMSE, the better a given model is able to ‚Äúfit‚Äù a dataset.\n  \n    *From our models, **SVD** performs the best!*\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Cross Validation","metadata":{}},{"cell_type":"code","source":"svd_val = SVD(n_epochs = 40, n_factors = 200, init_std_dev = 0.05, random_state = 42)\nCV_ = cross_validate(svd_val, data, measures = ['RMSE'], cv = 5, verbose = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {'n_epochs':[40],\n              'n_factors':[400], \n              'init_std_dev':[0.005], \n              'random_state':[42]} \ngrid_SVD = GridSearchCV(SVD, cv = 5, measures = ['rmse'], param_grid = param_grid, n_jobs = -1)\ngrid_SVD.fit(data)\nprint('Best score:\\t', grid_SVD.best_score['rmse'])\nprint('Best parameters:\\t', grid_SVD.best_params['rmse'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svd_test = SVD(n_epochs = 40, n_factors = 400, init_std_dev = 0.005, random_state=42)\nsvd_test.fit(trainset)\npredictions = svd_test.test(testset)\nrmse = accuracy.rmse(predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<b>Cross-validation:</b> By using Cross-Validation, we are able to get more metrics and draw important conclusion both about our algorithm and our data.\n</div>\n\n","metadata":{}},{"cell_type":"markdown","source":" <a id=\"eight\"></a>\n## 8. References\n<a href=#cont>Back to Table of Contents</a>\n\n| üìΩ *Description: References* üìΩ |\n| :--------------------------- |\n  In this section, we have listed all the refrences used throught this notebook for futher reading.\n  \n  ---\n  \n    \n1. [What Are Recommender System](https://www.analyticssteps.com/blogs/what-are-recommendation-systems-machine-learning)\n2. [Explained - Working and Advantages of a Recommendation Engine](https://medium.com/geekculture/explained-working-and-advantages-of-a-recommendation-engine-16cbff7796c)\n3. [(5) Reasons why you should use Cross-Validation](https://towardsdatascience.com/5-reasons-why-you-should-use-cross-validation-in-your-data-science-project-8163311a1e79)\n4. [SVD](https://scikit-learn.org/stable/modules/svm.html)\n5. [Collaborative Filtering](https://datasciencemadesimpler.wordpress.com/tag/co-clustering/)","metadata":{}},{"cell_type":"markdown","source":"                                                                   *FIN*  ","metadata":{}}]}